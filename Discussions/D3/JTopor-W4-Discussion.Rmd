---
title: "643 Discussion 3"
author: "James Topor"
date: "June 24, 2017"
output: html_document
---

### Problem Statement

*In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments.*

_____

### Bias in Recommender Systems

Recommender systems can reinforce human bias in a variety of ways, such as recommending items to users on the basis of their gender, income, physical location, age, or other identifying characteristics. The examples given in the "__When Recommendation Systems Go Bad__" presentation (https://www.youtube.com/watch?v=MqoRzNhrTnQ) represent very real instances of algorithms generating overtly biased recommendations. Unfortunately, the algorithms used to operationalize recommender systems are neither perfect nor infallible, and are, of course, designed and implemented by human beings who themselves are prone to many types of biases, both implicit and explicit. 

Furthermore, biases may be implicit within the data used as the basis of a recommender system. For example, the recommendations of any content-based recommendation system will inherently reflect the opinions and biases of one or more of the following:

- Those who label (or "tag" or "categorize") the items to be recommended;

- Those who authored any content that may have been algorithmically parsed into various recommender system categorizations;

- Those who designed the algorithms used to parse the authored content into various categorizations. 

Similarly, recommender systems based on either explicit or implicit ratings will be influenced by the biases and/or usage histories of the users of such systems.

_____

### "Fake" Reviews as a Source of Human Bias

An obvious example of bias in ratings-based recommender systems is the widespread presence of "fake" reviews that can be found on web sites such as __Amazon__ and __Yelp__. In many instances, these non-authentic reviews are the result of people being paid to either boost or diminish the popularity of a reviewed item by the proprietors or competitors of the item in question. An article on __TheWireCutter.com__ (see http://thewirecutter.com/blog/lets-talk-about-amazon-reviews/) summarizes the compensated view process as follows:

- *The compensated-review process is simple: Businesses paid to create dummy accounts purchase products from Amazon and write four- and five-star reviews. Buying the product makes it tougher for Amazon to police the reviews, because the reviews are in fact based on verified purchases. The dummy accounts buy and review all sorts of things, and some of the more savvy pay-for-review sites even have their faux reviewers pepper in a few negative reviews of products made and sold by brands that aren't clients to create a sense of "authenticity."*

Such non-authentic reviews are, in fact, a type of human bias that is being deliberately imposed on a recommender system by ill-intentioned actors for purposes of creating a form of bias within the context of the recommender system's output. While both __Yelp__ and __Amazon__ have repeatedly announced crackdowns on the authors of such reviews, their efforts have not been successful, as these relatively recent articles show:

- http://nypost.com/2017/05/19/scammers-elude-amazon-crackdown-on-fake-reviews-with-new-tricks/

- https://www.eater.com/2016/5/3/11578978/yelp-fake-reviews

Similar "fake review" problems are known to exist within the ratings we see for items on __Twitter__, __YouTube__, __Instagram__, news sites, and other platforms that rely on user feedback for purposes of making content recommendations.

_____

### Sampling Bias and Self-Selection Bias Within Recommender Systems

Setting aside the problem of "fake" reviews for the moment, many types of human bias that manifest themselves within the context of recommender systems are related to the statistical concepts of __sampling bias__ and __self-selection bias__. __Sampling bias__ can occur if a sample (e.g., of actual or likely recommender system users) is collected in such a way that some members of the actual target population are underrepresented. __Self-selection bias__ can happen when individuals opt to "select" themselves into a group, thereby resulting in a biased sample (the group) that is not representative of the actual population to which they belong. 

Within the context of a ratings-based recommender system, self-selection bias can easily occur as the result of some system users being more motivated to rate items than are other users: As a result, recommendations generated by such a system are likely to be highly influenced by the preferences and/or biases of the motivated users rather than being based upon the input of the population as a whole. Likewise, sampling bias can be induced by the designers of the recommender system through their decisions to target certain users for specific recommendations while other users are not recommended those same items: As a result, the under-sampled portion of the user base will be less likely to be exposed to such items, thereby potentially allowing the ratings of those items to be disproportionally based upon the input of those users who did receive recommendations for the items in question.

_____

### Conclusion

In light of these issues, I do not believe that recommender systems can help to prevent unethical targeting or customer segmentation very easily. For a recommender system to do so would require that the system be cognizant of each user's identifying characteristics (i.e., age, gender, ethnicity, locality, etc.) and also have algorithms in place that somehow prevent any user belonging to any such category from being unfairly segmented or targeted. Even if we assume that a recommender system could somehow aggregate all of the required identifying characteristics for any possible user, the chances of there being an algorithmic solution for the prevention of either unfair segmenting or targeting or self-selection bias or "fake" reviews seems rather remote.

Additionally, biases are often not easily quantifiable, and the appearance of bias can be highly subjective: What one individual finds to be an instance of objectionable bias might appear to another individual to be a completely unbiased occurrence. As such, it may prove to be impossible to completely eliminate all types of __perceived__ human bias from any recommender system. 